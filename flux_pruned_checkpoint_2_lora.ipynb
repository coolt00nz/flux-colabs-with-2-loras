{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcgJU6rA87Wg"
      },
      "outputs": [],
      "source": [
        "# To render using a Flux Pruned Checkpoint or LoRA model copied from your mounted Google Drive, run this cell first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRUNED FLUX DEV FP8 CONFIGURATION SETUP AND USAGE INSTRUCTIONS**\n",
        "- Requires a Pruned FLUX fp8 Checkpoint safetensor model (UNET only), ~11GB or smaller.\n",
        "- Choose one CHECKPOINT_SOURCE from menu below (googledrive, civitai, or huggingface).\n",
        "- If using googledrive as your CHECKPOINT_SOURCE, paste the googledrive path of your pre-downloaded checkpoint model into CHECKPOINT_DRIVE_PATH.\n",
        "- If using civitai as your CHECKPOINT_SOURCE, choose the civitai_checkpoint_option to either use a publicly available civitai model (no further action required) or enter your own civitai pruned fp8 checkpoint model download URL (paste into CHECKPOINT_CAI_URL).\n",
        "- If using huggingface as your CHECKPOINT_SOURCE, choose the hf_checkpoint_option to either use a publicly available huggingface model (no further action required) or enter your own huggingface pruned fp8 checkpoint model download URL (paste into CHECKPOINT_HF_URL).\n",
        "- Note most civitai and a few huggingface models (either checkpoint or LoRA) require an API token for direct download to the Colab.  Paste in either or both API tokens below, and they will automatically be added to your checkpoint or LoRA download URLs.  (API tokens not necessary if using either a pre-downloaded or publicly available checkpoint model and no LoRAs.)\n",
        "- To use one or two optional LoRAs in your render, you can either download them from a Civitai or Hugging Face URL that you paste into a text box in the Gradio interface, or you can copy over a pre-downloaded LoRA from your mounted Google Drive by selecting it through a dropdown menu in the Gradio Interface.\n",
        "- To use LoRAs from googledrive, check the use_drive_loras box and paste in your googledrive LoRA directory path into LORA_DRIVE_DIR.\n",
        "- You can potentially use different LoRA sources for each LoRA, no matter what choice you make for CHECKPOINT_SOURCE, and you can change the LoRAs to be used in each render through the Gradio Interface without restarting the Colab.  Once a LoRA has been downloaded, it will be saved for the duration of the run, so it will load faster, for shorter render times, if you reuse it.\n",
        "- Renders take ~3min using default settings and one LoRA.  Renders without LoRAs will be faster, and renders using two LoRAs will be slower.  If you want to experiment, this colab has also run successfully using Schnell or Hybrid safetensor checkpoint models requiring fewer steps\n",
        "- BTW, seeds used for renders appear below in this colab, in case you want to adjust parameters in the Gradio Interface and rerun a render using the same seed.\n",
        "- Have fun rendering!"
      ],
      "metadata": {
        "id": "D2lupAO0HdyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_section_cell"
      },
      "outputs": [],
      "source": [
        "# @title Pruned Flux Configuration Setup\n",
        "\n",
        "# Checkpoint Source Selection\n",
        "CHECKPOINT_SOURCE = \"civitai\" # @param [\"googledrive\", \"civitai\", \"huggingface\"]\n",
        "\n",
        "# Google Drive Configuration\n",
        "CHECKPOINT_DRIVE_PATH = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Civitai Configuration\n",
        "civitai_checkpoint_option = \"Use publicly available model (Flux Unchained)\" # @param [\"Use publicly available model (Flux Unchained)\", \"Paste custom URL below\"]\n",
        "CHECKPOINT_CAI_URL = \"https://civitai.com/api/download/models/742989\" # @param {type:\"string\"}\n",
        "\n",
        "if civitai_checkpoint_option == \"Use publicly available model (Flux Unchained)\":\n",
        "    CHECKPOINT_CAI_URL = \"https://civitai.com/api/download/models/742989\"\n",
        "else:\n",
        "    CHECKPOINT_CAI_URL = CHECKPOINT_CAI_URL.split(\"?\")[0]  # Clean URL\n",
        "\n",
        "# Hugging Face Configuration  \n",
        "hf_checkpoint_option = \"Use publicly available model\" # @param [\"Use publicly available model\", \"Paste custom URL below\"]\n",
        "CHECKPOINT_HF_URL = \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors\" # @param {type:\"string\"}\n",
        "\n",
        "if hf_checkpoint_option == \"Use publicly available model\":\n",
        "    CHECKPOINT_HF_URL = \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors\"\n",
        "else:\n",
        "    CHECKPOINT_HF_URL = CHECKPOINT_HF_URL.split(\"?\")[0]  # Clean URL\n",
        "\n",
        "# API Tokens\n",
        "CIVITAI_TOKEN = \"\" # @param {type:\"string\"}\n",
        "HF_TOKEN = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Default to placeholder values if empty\n",
        "CIVITAI_TOKEN = CIVITAI_TOKEN if CIVITAI_TOKEN else \"YOUR_CIVITAI_TOKEN_HERE\"\n",
        "HF_TOKEN = HF_TOKEN if HF_TOKEN else \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
        "\n",
        "# LoRA Configuration\n",
        "use_drive_loras = False # @param {type:\"boolean\"}\n",
        "LORA_DRIVE_DIR = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CONFIGURATION SUMMARY:\")\n",
        "print(f\"✓ Checkpoint Source: {CHECKPOINT_SOURCE}\")\n",
        "if CHECKPOINT_SOURCE == \"civitai\":\n",
        "    print(f\"✓ Civitai URL: {CHECKPOINT_CAI_URL}\")\n",
        "    print(f\"✓ Civitai Token: {'Provided' if CIVITAI_TOKEN != 'YOUR_CIVITAI_TOKEN_HERE' else 'Not provided'}\")\n",
        "elif CHECKPOINT_SOURCE == \"huggingface\":\n",
        "    print(f\"✓ HuggingFace URL: {CHECKPOINT_HF_URL}\")\n",
        "    print(f\"✓ HuggingFace Token: {'Provided' if HF_TOKEN != 'YOUR_HUGGINGFACE_TOKEN_HERE' else 'Not provided'}\")\n",
        "else:\n",
        "    print(f\"✓ Google Drive Path: {CHECKPOINT_DRIVE_PATH if CHECKPOINT_DRIVE_PATH else 'Not specified'}\")\n",
        "\n",
        "if use_drive_loras and LORA_DRIVE_DIR:\n",
        "    print(f\"✓ LoRA Directory: {LORA_DRIVE_DIR}\")\n",
        "else:\n",
        "    print(\"✓ LoRA Directory: Not configured\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaFqRYHc9H_K"
      },
      "outputs": [],
      "source": [
        "# @title After configuration, change runtime type to T4 GPU, run this cell for setup, ~7min, then launch the link for the Gradio Interface appearing at the bottom.\n",
        "\n",
        "%cd /content\n",
        "!git clone -b totoro4 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post1 gradio==4.44.1 python-multipart==0.0.12\n",
        "!pip install pydantic==2.9.2\n",
        "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_flux\n",
        "from totoro import model_management\n",
        "import gradio as gr\n",
        "import requests\n",
        "import shutil\n",
        "import totoro.utils\n",
        "\n",
        "\n",
        "def patched_flux_to_diffusers(mmdit_config, output_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Enhanced Flux model key mapping for LoRA compatibility\n",
        "    Maps from source components to combined linear1\n",
        "    \"\"\"\n",
        "    n_double_layers = mmdit_config.get(\"depth\", 0)\n",
        "    n_single_layers = mmdit_config.get(\"depth_single_blocks\", 0)\n",
        "    hidden_size = mmdit_config.get(\"hidden_size\", 0)\n",
        "\n",
        "    key_map = {}\n",
        "\n",
        "    # First map transformer blocks - these correspond to the double blocks\n",
        "    for index in range(n_double_layers):\n",
        "        prefix = f\"{output_prefix}double_blocks.{index}\"\n",
        "        prefix_from = f\"transformer_blocks.{index}\"\n",
        "\n",
        "        # Handle attention components with proper tensor shapes\n",
        "        for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "            clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "            # Map main attention components\n",
        "            qkv_map = {\n",
        "                f\"{prefix_from}.attn.to_q{end}\": (f\"{prefix}.img_attn.qkv{clean_end}\", (0, 0, hidden_size)),\n",
        "                f\"{prefix_from}.attn.to_k{end}\": (f\"{prefix}.img_attn.qkv{clean_end}\", (0, hidden_size, hidden_size)),\n",
        "                f\"{prefix_from}.attn.to_v{end}\": (f\"{prefix}.img_attn.qkv{clean_end}\", (0, hidden_size * 2, hidden_size)),\n",
        "                f\"{prefix_from}.attn.add_q_proj{end}\": (f\"{prefix}.txt_attn.qkv{clean_end}\", (0, 0, hidden_size)),\n",
        "                f\"{prefix_from}.attn.add_k_proj{end}\": (f\"{prefix}.txt_attn.qkv{clean_end}\", (0, hidden_size, hidden_size)),\n",
        "                f\"{prefix_from}.attn.add_v_proj{end}\": (f\"{prefix}.txt_attn.qkv{clean_end}\", (0, hidden_size * 2, hidden_size)),\n",
        "            }\n",
        "            key_map.update(qkv_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in qkv_map.items()})\n",
        "\n",
        "            # Map output projections without splits\n",
        "            proj_map = {\n",
        "                f\"{prefix_from}.attn.to_out.0{end}\": f\"{prefix}.img_attn.proj{clean_end}\",\n",
        "                f\"{prefix_from}.attn.to_add_out{end}\": f\"{prefix}.txt_attn.proj{clean_end}\",\n",
        "            }\n",
        "            key_map.update(proj_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in proj_map.items()})\n",
        "\n",
        "        # Map MLP components\n",
        "        for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "            clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "            mlp_map = {\n",
        "                f\"{prefix_from}.ff.net.0.proj{end}\": f\"{prefix}.img_mlp.0{clean_end}\",\n",
        "                f\"{prefix_from}.ff.net.2{end}\": f\"{prefix}.img_mlp.2{clean_end}\",\n",
        "                f\"{prefix_from}.ff_context.net.0.proj{end}\": f\"{prefix}.txt_mlp.0{clean_end}\",\n",
        "                f\"{prefix_from}.ff_context.net.2{end}\": f\"{prefix}.txt_mlp.2{clean_end}\",\n",
        "                f\"{prefix_from}.norm1.linear{end}\": f\"{prefix}.img_mod.lin{clean_end}\",\n",
        "                f\"{prefix_from}.norm1_context.linear{end}\": f\"{prefix}.txt_mod.lin{clean_end}\",\n",
        "            }\n",
        "            key_map.update(mlp_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in mlp_map.items()})\n",
        "\n",
        "    # Map single transformer blocks with combined linear1\n",
        "    for index in range(n_single_layers):\n",
        "        prefix = f\"{output_prefix}single_blocks.{index}\"\n",
        "        prefix_from = f\"single_transformer_blocks.{index}\"\n",
        "\n",
        "        # Handle components with LoRA variants\n",
        "        for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "            clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "\n",
        "            # Map the attention components to sections of linear1\n",
        "            linear1 = f\"{prefix}.linear1{clean_end}\"\n",
        "            key_map[f\"{prefix_from}.attn.to_q{end}\"] = (linear1, (0, 0, hidden_size))\n",
        "            key_map[f\"{prefix_from}.attn.to_k{end}\"] = (linear1, (0, hidden_size, hidden_size))\n",
        "            key_map[f\"{prefix_from}.attn.to_v{end}\"] = (linear1, (0, hidden_size * 2, hidden_size))\n",
        "            key_map[f\"{prefix_from}.proj_mlp{end}\"] = (linear1, (0, hidden_size * 3, hidden_size * 4))\n",
        "\n",
        "            # Also map transformer prefixed versions\n",
        "            key_map[f\"transformer.{prefix_from}.attn.to_q{end}\"] = (linear1, (0, 0, hidden_size))\n",
        "            key_map[f\"transformer.{prefix_from}.attn.to_k{end}\"] = (linear1, (0, hidden_size, hidden_size))\n",
        "            key_map[f\"transformer.{prefix_from}.attn.to_v{end}\"] = (linear1, (0, hidden_size * 2, hidden_size))\n",
        "            key_map[f\"transformer.{prefix_from}.proj_mlp{end}\"] = (linear1, (0, hidden_size * 3, hidden_size * 4))\n",
        "\n",
        "            # Map other components\n",
        "            other_map = {\n",
        "                f\"{prefix_from}.proj_out{end}\": f\"{prefix}.linear2{clean_end}\",\n",
        "                f\"{prefix_from}.norm.linear{end}\": f\"{prefix}.modulation.lin{clean_end}\",\n",
        "            }\n",
        "            key_map.update(other_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in other_map.items()})\n",
        "\n",
        "    # Map base model components\n",
        "    for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "        clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "        base_map = {\n",
        "            f\"context_embedder{end}\": f\"{output_prefix}txt_in{clean_end}\",\n",
        "            f\"time_text_embed.timestep_embedder.linear_1{end}\": f\"{output_prefix}time_in.in_layer{clean_end}\",\n",
        "            f\"time_text_embed.timestep_embedder.linear_2{end}\": f\"{output_prefix}time_in.out_layer{clean_end}\",\n",
        "            f\"time_text_embed.text_embedder.linear_1{end}\": f\"{output_prefix}vector_in.in_layer{clean_end}\",\n",
        "            f\"time_text_embed.text_embedder.linear_2{end}\": f\"{output_prefix}vector_in.out_layer{clean_end}\",\n",
        "            f\"proj_out{end}\": f\"{output_prefix}final_layer.linear{clean_end}\",\n",
        "        }\n",
        "        key_map.update(base_map)\n",
        "        key_map.update({f\"transformer.{k}\": v for k, v in base_map.items()})\n",
        "\n",
        "    return key_map\n",
        "\n",
        "# Replace the original function\n",
        "totoro.utils.flux_to_diffusers = patched_flux_to_diffusers\n",
        "\n",
        "print(\"Patched Flux LoRA compatibility layer with correct component mapping\")\n",
        "\n",
        "\n",
        "# Helper function to check if a token is valid\n",
        "def is_valid_token(token):\n",
        "    placeholder_values = [\"YOUR_TOKEN_HERE\", \"YOUR_HUGGINGFACE_TOKEN_HERE\", \"\", None]\n",
        "    return token not in placeholder_values\n",
        "\n",
        "# Helper function to list available LoRAs from Google Drive\n",
        "def list_drive_loras(lora_dir):\n",
        "    if not os.path.exists(lora_dir):\n",
        "        return []\n",
        "    return [f for f in os.listdir(lora_dir) if f.endswith('.safetensors')]\n",
        "\n",
        "# Modified checkpoint loading logic\n",
        "if CHECKPOINT_SOURCE.lower() == \"googledrive\":\n",
        "    if not os.path.exists(CHECKPOINT_DRIVE_PATH):\n",
        "        raise Exception(f\"Checkpoint not found at: {CHECKPOINT_DRIVE_PATH}\")\n",
        "    # Copy checkpoint to ComfyUI directory\n",
        "    checkpoint_dest = \"/content/TotoroUI/models/unet/pruned-checkpoint.safetensors\"\n",
        "    os.makedirs(os.path.dirname(checkpoint_dest), exist_ok=True)\n",
        "    shutil.copy2(CHECKPOINT_DRIVE_PATH, checkpoint_dest)\n",
        "    print(f\"Copied checkpoint from Google Drive to: {checkpoint_dest}\")\n",
        "elif CHECKPOINT_SOURCE.lower() == \"civitai\":\n",
        "    if not is_valid_token(CIVITAI_TOKEN):\n",
        "        print(\"Warning: No Civitai token provided. Attempting download in case model is public...\")\n",
        "    download_url = f\"{CHECKPOINT_CAI_URL}{'?token=' + CIVITAI_TOKEN if is_valid_token(CIVITAI_TOKEN) else ''}\"\n",
        "    download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d /content/TotoroUI/models/unet -o pruned-checkpoint.safetensors'\n",
        "    os.system(download_cmd)\n",
        "else:  # huggingface\n",
        "    if not is_valid_token(HF_TOKEN):\n",
        "        print(\"Warning: No Hugging Face token provided. Attempting download in case model is public...\")\n",
        "    download_url = CHECKPOINT_HF_URL\n",
        "    download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d /content/TotoroUI/models/unet -o pruned-checkpoint.safetensors'\n",
        "    if is_valid_token(HF_TOKEN):\n",
        "        download_cmd += f' --header=\"Authorization: Bearer {HF_TOKEN}\"'\n",
        "    os.system(download_cmd)\n",
        "\n",
        "# Download other required models\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
        "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "\n",
        "with torch.inference_mode():\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"pruned-checkpoint.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "downloaded_loras = {}\n",
        "\n",
        "# Modified download_lora function with filename collision handling\n",
        "def download_lora(url, source):\n",
        "    global downloaded_loras\n",
        "\n",
        "    cache_key = f\"{source}:{url}\"\n",
        "    if cache_key in downloaded_loras:\n",
        "        if os.path.exists(downloaded_loras[cache_key]):\n",
        "            print(f\"Using cached LoRA: {downloaded_loras[cache_key]}\")\n",
        "            return downloaded_loras[cache_key]\n",
        "        else:\n",
        "            # Cached file no longer exists, remove from cache\n",
        "            del downloaded_loras[cache_key]\n",
        "\n",
        "    def get_unique_filepath(base_path, filename):\n",
        "        \"\"\"Generate unique filepath by appending source and counter if needed\"\"\"\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        filepath = os.path.join(base_path, filename)\n",
        "        counter = 1\n",
        "\n",
        "        while os.path.exists(filepath):\n",
        "            # Check if it's literally the same file\n",
        "            if any(existing_path == filepath for existing_path in downloaded_loras.values()):\n",
        "                print(f\"Reusing identical LoRA path: {filepath}\")\n",
        "                return filepath\n",
        "\n",
        "            new_name = f\"{name}_{source}_{counter}{ext}\"\n",
        "            filepath = os.path.join(base_path, new_name)\n",
        "            counter += 1\n",
        "\n",
        "        return filepath\n",
        "\n",
        "    try:\n",
        "        if source.lower() == \"googledrive\":\n",
        "            # For Google Drive, url is actually the path to the LoRA file\n",
        "            if not os.path.exists(url):\n",
        "                raise Exception(f\"LoRA not found at: {url}\")\n",
        "            filename = os.path.basename(url)\n",
        "            filepath = get_unique_filepath('/content/TotoroUI/models/loras', filename)\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "            shutil.copy2(url, filepath)\n",
        "            print(f\"LoRA path: {filepath}\")\n",
        "            downloaded_loras[cache_key] = filepath\n",
        "            return filepath\n",
        "        elif source.lower() == \"civitai\":\n",
        "            if not is_valid_token(CIVITAI_TOKEN):\n",
        "                print(\"Warning: No Civitai token provided. Attempting LoRA download in case model is public...\")\n",
        "            headers = {\"Authorization\": f\"Bearer {CIVITAI_TOKEN}\"} if is_valid_token(CIVITAI_TOKEN) else {}\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                content_disposition = response.headers.get('Content-Disposition')\n",
        "                if content_disposition:\n",
        "                    filename = content_disposition.split('filename=')[-1].strip('\"').split(\"?\")[0]\n",
        "                else:\n",
        "                    filename = f\"lora_{len(downloaded_loras)}.safetensors\"\n",
        "                filepath = get_unique_filepath('/content/TotoroUI/models/loras', filename)\n",
        "                os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "                download_url = f\"{url}{'?token=' + CIVITAI_TOKEN if is_valid_token(CIVITAI_TOKEN) else ''}\"\n",
        "\n",
        "                download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d /content/TotoroUI/models/loras -o \"{os.path.basename(filepath)}\"'\n",
        "                os.system(download_cmd)\n",
        "\n",
        "                if not os.path.exists(filepath):\n",
        "                    raise Exception(f\"Download failed: {filename} not found. If this is a private model, make sure to provide the appropriate token.\")\n",
        "\n",
        "                print(f\"LoRA downloaded to: {filepath}\")\n",
        "                downloaded_loras[cache_key] = filepath\n",
        "                return filepath\n",
        "        else:  # huggingface\n",
        "            os.makedirs(\"/content/TotoroUI/models/loras\", exist_ok=True)\n",
        "            filename = url.split(\"/\")[-1] # Filename at end of HF URL\n",
        "            filepath = get_unique_filepath('/content/TotoroUI/models/loras', filename)\n",
        "\n",
        "            download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d /content/TotoroUI/models/loras -o \"{os.path.basename(filepath)}\"'\n",
        "            if is_valid_token(HF_TOKEN):\n",
        "                download_cmd += f' --header=\"Authorization: Bearer {HF_TOKEN}\"'\n",
        "            else:\n",
        "                print(\"Warning: No Hugging Face token provided. Attempting LoRA download in case model is public...\")\n",
        "\n",
        "            os.system(download_cmd)\n",
        "\n",
        "            if not os.path.exists(filepath):\n",
        "                raise Exception(f\"Download failed: {filename} not found. If this is a private model, make sure to provide the appropriate token.\")\n",
        "\n",
        "            print(f\"LoRA downloaded to: {filepath}\")\n",
        "            downloaded_loras[cache_key] = filepath\n",
        "            return filepath\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading LoRA: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance,\n",
        "             lora1_source, lora1_url, lora1_strength_model, lora1_strength_clip,\n",
        "             lora2_source, lora2_url, lora2_strength_model, lora2_strength_clip, clip_skip):\n",
        "    global unet, clip, vae\n",
        "\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(f\"Using seed: {seed}\")\n",
        "\n",
        "    try:\n",
        "        # LoRA handling\n",
        "        unet_lora, clip_lora = unet, clip\n",
        "\n",
        "        if lora1_url:\n",
        "            lora1_path = download_lora(lora1_url.split(\"?\")[0], lora1_source)\n",
        "            lora1_filename = os.path.basename(lora1_path)\n",
        "            print(f\"Using LoRA 1: {lora1_filename} from {lora1_source}\")\n",
        "            unet_lora, clip_lora = LoraLoader.load_lora(unet_lora, clip_lora, lora1_filename, lora1_strength_model, lora1_strength_clip)\n",
        "\n",
        "        if lora2_url:\n",
        "            lora2_path = download_lora(lora2_url.split(\"?\")[0], lora2_source)\n",
        "            lora2_filename = os.path.basename(lora2_path)\n",
        "            print(f\"Using LoRA 2: {lora2_filename} from {lora2_source}\")\n",
        "            unet_lora, clip_lora = LoraLoader.load_lora(unet_lora, clip_lora, lora2_filename, lora2_strength_model, lora2_strength_clip)\n",
        "\n",
        "        if not lora1_url and not lora2_url:\n",
        "            print(\"No LoRA URLs provided, using base model\")\n",
        "\n",
        "        # Encode the prompt\n",
        "        tokens = clip_lora.tokenize(positive_prompt)\n",
        "        cond, pooled = clip_lora.encode_from_tokens(tokens, return_pooled=True)\n",
        "\n",
        "        # Apply CLIP skip if needed\n",
        "        if clip_skip > 0:\n",
        "            cond = cond[:, :cond.shape[1] - clip_skip]\n",
        "\n",
        "        # Create proper conditioning structure expected by FluxGuidance\n",
        "        cond_dict = {\"pooled_output\": pooled, \"guidance\": guidance}\n",
        "        conditioning = [[cond, cond_dict]]\n",
        "\n",
        "        # Set up sampling parameters\n",
        "        noise = RandomNoise.get_noise(seed)[0]\n",
        "        guider = BasicGuider.get_guider(unet_lora, conditioning)[0]\n",
        "        sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "        sigmas = BasicScheduler.get_sigmas(unet_lora, scheduler, steps, 1.0)[0]\n",
        "\n",
        "        # Create latent noise image\n",
        "        latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "\n",
        "        # Perform sampling\n",
        "        sample, _ = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "        model_management.soft_empty_cache()\n",
        "\n",
        "        # Decode with normalized range\n",
        "        decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "        image_array = np.array(decoded * 255, dtype=np.uint8)[0]\n",
        "        Image.fromarray(image_array).save(\"/content/flux.png\")\n",
        "        return \"/content/flux.png\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Modified Gradio interface with complete UI handling\n",
        "with gr.Blocks(analytics_enabled=False) as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            positive_prompt = gr.Textbox(lines=3, interactive=True,\n",
        "                                       value=\"Intergalactic beauty contest winner, pretty female humanoid alien in party dress, holding up sign saying 'UNET Flux With 2 LoRA Colab'\",\n",
        "                                       label=\"Prompt\")\n",
        "            width = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"width\")\n",
        "            height = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"height\")\n",
        "            seed = gr.Slider(minimum=0, maximum=18446744073709551615, value=0, step=1, label=\"seed (0=random)\")\n",
        "            steps = gr.Slider(minimum=4, maximum=50, value=20, step=1, label=\"steps\")\n",
        "            guidance = gr.Slider(minimum=0, maximum=20, value=3.5, step=0.5, label=\"guidance\")\n",
        "            sampler_name = gr.Dropdown([\"euler\", \"heun\", \"heunpp2\", \"lms\", \"dpm_2\", \"dpmpp_2m\", \"deis\", \"ddim\", \"uni_pc\", \"uni_pc_bh2\"],\n",
        "                                     label=\"sampler_name\", value=\"dpmpp_2m\")\n",
        "            scheduler = gr.Dropdown([\"normal\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"],\n",
        "                                  label=\"scheduler\", value=\"sgm_uniform\")\n",
        "\n",
        "            # Modified LoRA 1 settings with Google Drive support\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### LoRA 1 Settings\")\n",
        "                lora1_source = gr.Radio([\"civitai\", \"huggingface\", \"googledrive\"],\n",
        "                                      label=\"LoRA 1 Source\", value=\"civitai\")\n",
        "                lora1_drive_files = gr.Dropdown(choices=[], label=\"Select LoRA 1 from Drive\", visible=False)\n",
        "                lora1_url = gr.Textbox(lines=1, interactive=True, value=\"\", label=\"LoRA 1 URL/Path\")\n",
        "                lora1_strength_model = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                               label=\"LoRA 1 Strength (Model)\")\n",
        "                lora1_strength_clip = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                              label=\"LoRA 1 Strength (CLIP)\")\n",
        "\n",
        "            # Modified LoRA 2 settings with Google Drive support\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### LoRA 2 Settings\")\n",
        "                lora2_source = gr.Radio([\"civitai\", \"huggingface\", \"googledrive\"],\n",
        "                                      label=\"LoRA 2 Source\", value=\"civitai\")\n",
        "                lora2_drive_files = gr.Dropdown(choices=[], label=\"Select LoRA 2 from Drive\", visible=False)\n",
        "                lora2_url = gr.Textbox(lines=1, interactive=True, value=\"\", label=\"LoRA 2 URL/Path\")\n",
        "                lora2_strength_model = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                               label=\"LoRA 2 Strength (Model)\")\n",
        "                lora2_strength_clip = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                              label=\"LoRA 2 Strength (CLIP)\")\n",
        "\n",
        "            clip_skip = gr.Slider(minimum=0, maximum=2, value=0, step=1, label=\"CLIP Skip\")\n",
        "            generate_button = gr.Button(\"Generate\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_image = gr.Image(label=\"Generated image\", interactive=False)\n",
        "\n",
        "    # Updated interface update functions\n",
        "    def update_lora_interface(source, url_textbox):\n",
        "        \"\"\"Update LoRA interface based on selected source\"\"\"\n",
        "        if source == \"googledrive\" and 'LORA_DRIVE_DIR' in globals():\n",
        "            return (\n",
        "                gr.update(visible=True, choices=list_drive_loras(LORA_DRIVE_DIR), value=None),\n",
        "                gr.update(visible=True, interactive=False, value=\"\")\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                gr.update(visible=False, choices=[], value=None),\n",
        "                gr.update(visible=True, interactive=True, value=\"\")\n",
        "            )\n",
        "\n",
        "    def update_lora_url(selected_file, source):  # Added source parameter\n",
        "        \"\"\"Update URL when a file is selected from dropdown\"\"\"\n",
        "        if selected_file and source == \"googledrive\":  # Only modify interactivity for Google Drive\n",
        "            full_path = f\"{LORA_DRIVE_DIR}/{selected_file}\"\n",
        "            return gr.update(value=full_path, visible=True, interactive=False)\n",
        "        return gr.update(value=\"\", visible=True)  # Maintain current interactive state for other sources\n",
        "\n",
        "    # Set up LoRA 1 interface events\n",
        "    lora1_source.change(\n",
        "        fn=update_lora_interface,\n",
        "        inputs=[lora1_source, lora1_url],\n",
        "        outputs=[lora1_drive_files, lora1_url]\n",
        "    )\n",
        "    lora1_drive_files.change(\n",
        "        fn=update_lora_url,\n",
        "        inputs=[lora1_drive_files, lora1_source],  # Added source input\n",
        "        outputs=[lora1_url]\n",
        "    )\n",
        "\n",
        "    # Set up LoRA 2 interface events\n",
        "    lora2_source.change(\n",
        "        fn=update_lora_interface,\n",
        "        inputs=[lora2_source, lora2_url],\n",
        "        outputs=[lora2_drive_files, lora2_url]\n",
        "    )\n",
        "    lora2_drive_files.change(\n",
        "        fn=update_lora_url,\n",
        "        inputs=[lora2_drive_files, lora2_source],  # Added source input\n",
        "        outputs=[lora2_url]\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        fn=generate,\n",
        "        inputs=[\n",
        "            positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance,\n",
        "            lora1_source, lora1_url, lora1_strength_model, lora1_strength_clip,\n",
        "            lora2_source, lora2_url, lora2_strength_model, lora2_strength_clip, clip_skip\n",
        "        ],\n",
        "        outputs=output_image\n",
        "    )\n",
        "\n",
        "demo.queue().launch(inline=False, share=True, debug=True)"
      ],
      "metadata": {
        "id": "CaFqRYHc9H_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
