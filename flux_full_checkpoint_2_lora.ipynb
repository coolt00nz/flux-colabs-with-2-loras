{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcgJU6rA87Wg"
      },
      "outputs": [],
      "source": [
        "# To render using a Flux Pruned Checkpoint or LoRA model copied from your mounted Google Drive, run this cell first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for use\n",
        "# 1. Fill out Configuration Section\n",
        "# 2. Change runtime type to T4 GPU and connect\n",
        "# 3. Run this cell and wait for Gradio link to appear at bottom ~7min\n",
        "# 4. Launch link and enjoy creating Flux images for several hours\n",
        "\n",
        "\n",
        "# Configuration Section\n",
        "# Checkpoint Requirements - Full FLUX fp8 Checkpoint safetensor model (UNET, VAE, and CLIP baked in) ~16GB or smaller\n",
        "# Uncomment one Checkpoint Source Option below, and uncomment and paste in one URL/Path for the Checkpoint model, consistent with your Source Option choice.\n",
        "# Note most Civitai and some Hugging Face Checkpoint models required an API token for URL download to Colab (paste in below, token automatically added to download URL).\n",
        "# Or you can uncomment and download a publicly available Civitai or Hugging Face Checkpoint model (no URL paste in and no API Token required).\n",
        "\n",
        "#Checkpoint Sources - Uncomment one\n",
        "CHECKPOINT_SOURCE = \"civitai\"\n",
        "#CHECKPOINT_SOURCE = \"huggingface\"\n",
        "#CHECKPOINT_SOURCE = \"googledrive\"\n",
        "\n",
        "#If Civitai Checkpoint Source - Uncomment one and paste in URL or use publicly available\n",
        "#CHECKPOINT_CAI_URL = \"Paste in a Civitai checkpoint download URL here\".split(\"?\")[0]  #Additional Civitai API token may be required\n",
        "CHECKPOINT_CAI_URL = \"https://civitai.com/api/download/models/722828\"  #Publicly available Civitai model Flux Unchained\n",
        "\n",
        "#If Hugging Face Checkpoint Source - Uncomment one and paste in URL or use publicly available\n",
        "#CHECKPOINT_HF_URL = \"Paste in a Huggingface checkpoint download URL here\".split(\"?\")[0] #Additional Hugging Face API token may be required\n",
        "#CHECKPOINT_HF_URL = \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8-all-in-one.safetensors\"  #Publicly available Hugging Face model\n",
        "\n",
        "#If Google Drive Checkpoint Source - Uncomment and paste in Path\n",
        "#CHECKPOINT_DRIVE_PATH = \"Copy path to a Checkpoint model in your mounted Google Drive and paste here\"\n",
        "\n",
        "\n",
        "# API Token, required for most Civitai URL downloads directly to Colab, either Checkpoint or LoRA.\n",
        "# Also some Hugging Face Checkpoint or LoRA models require you to get usage permission from the developer, connected to your API token.\n",
        "# You also can decide not to edit either/both of the tokens below and still try downloading using a Civitai or Hugging Face Checkpoint or LoRA URL.  This may or may not work.\n",
        "CIVITAI_TOKEN = \"YOUR_CIVITAI_TOKEN_HERE\"\n",
        "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
        "\n",
        "\n",
        "# To use one or two optional LoRAs in your render, you can either download them from a Civitai or Hugging Face URL that you paste into a text box in your Gradio interface, or\n",
        "# you can copy over a LoRA from your mounted Google Drive by selecting it through a dropdown menu in your Gradio Interface.\n",
        "# You can use different LoRA sources for each LoRA, and you can change the LoRAs to be used in each render through the Gradio Interface without restarting the Colab.\n",
        "# Note you can also use LoRAs from your Google Drive, even if your Checkpoint Source is Civitai or Hugging Face, for example, a publicly available checkpoint model.\n",
        "\n",
        "#To use LoRAs from Google Drive - Uncomment and paste in Path\n",
        "#LORA_DRIVE_DIR = \"Copy path to a LoRA directory in your mounted Google Drive and paste here\"\n",
        "# End of Configuration Section\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!git clone -b totoro4 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post1 gradio==4.44.1 python-multipart==0.0.12\n",
        "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_flux\n",
        "from totoro import model_management\n",
        "import gradio as gr\n",
        "import requests\n",
        "import shutil\n",
        "import totoro.utils\n",
        "\n",
        "\n",
        "def patched_flux_to_diffusers(mmdit_config, output_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Enhanced Flux model key mapping for LoRA compatibility\n",
        "    Maps from source components to combined linear1\n",
        "    \"\"\"\n",
        "    n_double_layers = mmdit_config.get(\"depth\", 0)\n",
        "    n_single_layers = mmdit_config.get(\"depth_single_blocks\", 0)\n",
        "    hidden_size = mmdit_config.get(\"hidden_size\", 0)\n",
        "\n",
        "    key_map = {}\n",
        "\n",
        "    # First map transformer blocks - these correspond to the double blocks\n",
        "    for index in range(n_double_layers):\n",
        "        prefix = f\"{output_prefix}double_blocks.{index}\"\n",
        "        prefix_from = f\"transformer_blocks.{index}\"\n",
        "\n",
        "        # Handle attention components with proper tensor shapes\n",
        "        for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "            clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "            # Map main attention components\n",
        "            qkv_map = {\n",
        "                f\"{prefix_from}.attn.to_q{end}\": (f\"{prefix}.img_attn.qkv{clean_end}\", (0, 0, hidden_size)),\n",
        "                f\"{prefix_from}.attn.to_k{end}\": (f\"{prefix}.img_attn.qkv{clean_end}\", (0, hidden_size, hidden_size)),\n",
        "                f\"{prefix_from}.attn.to_v{end}\": (f\"{prefix}.img_attn.qkv{clean_end}\", (0, hidden_size * 2, hidden_size)),\n",
        "                f\"{prefix_from}.attn.add_q_proj{end}\": (f\"{prefix}.txt_attn.qkv{clean_end}\", (0, 0, hidden_size)),\n",
        "                f\"{prefix_from}.attn.add_k_proj{end}\": (f\"{prefix}.txt_attn.qkv{clean_end}\", (0, hidden_size, hidden_size)),\n",
        "                f\"{prefix_from}.attn.add_v_proj{end}\": (f\"{prefix}.txt_attn.qkv{clean_end}\", (0, hidden_size * 2, hidden_size)),\n",
        "            }\n",
        "            key_map.update(qkv_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in qkv_map.items()})\n",
        "\n",
        "            # Map output projections without splits\n",
        "            proj_map = {\n",
        "                f\"{prefix_from}.attn.to_out.0{end}\": f\"{prefix}.img_attn.proj{clean_end}\",\n",
        "                f\"{prefix_from}.attn.to_add_out{end}\": f\"{prefix}.txt_attn.proj{clean_end}\",\n",
        "            }\n",
        "            key_map.update(proj_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in proj_map.items()})\n",
        "\n",
        "        # Map MLP components\n",
        "        for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "            clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "            mlp_map = {\n",
        "                f\"{prefix_from}.ff.net.0.proj{end}\": f\"{prefix}.img_mlp.0{clean_end}\",\n",
        "                f\"{prefix_from}.ff.net.2{end}\": f\"{prefix}.img_mlp.2{clean_end}\",\n",
        "                f\"{prefix_from}.ff_context.net.0.proj{end}\": f\"{prefix}.txt_mlp.0{clean_end}\",\n",
        "                f\"{prefix_from}.ff_context.net.2{end}\": f\"{prefix}.txt_mlp.2{clean_end}\",\n",
        "                f\"{prefix_from}.norm1.linear{end}\": f\"{prefix}.img_mod.lin{clean_end}\",\n",
        "                f\"{prefix_from}.norm1_context.linear{end}\": f\"{prefix}.txt_mod.lin{clean_end}\",\n",
        "            }\n",
        "            key_map.update(mlp_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in mlp_map.items()})\n",
        "\n",
        "    # Map single transformer blocks with combined linear1\n",
        "    for index in range(n_single_layers):\n",
        "        prefix = f\"{output_prefix}single_blocks.{index}\"\n",
        "        prefix_from = f\"single_transformer_blocks.{index}\"\n",
        "\n",
        "        # Handle components with LoRA variants\n",
        "        for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "            clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "\n",
        "            # Map the attention components to sections of linear1\n",
        "            linear1 = f\"{prefix}.linear1{clean_end}\"\n",
        "            key_map[f\"{prefix_from}.attn.to_q{end}\"] = (linear1, (0, 0, hidden_size))\n",
        "            key_map[f\"{prefix_from}.attn.to_k{end}\"] = (linear1, (0, hidden_size, hidden_size))\n",
        "            key_map[f\"{prefix_from}.attn.to_v{end}\"] = (linear1, (0, hidden_size * 2, hidden_size))\n",
        "            key_map[f\"{prefix_from}.proj_mlp{end}\"] = (linear1, (0, hidden_size * 3, hidden_size * 4))\n",
        "\n",
        "            # Also map transformer prefixed versions\n",
        "            key_map[f\"transformer.{prefix_from}.attn.to_q{end}\"] = (linear1, (0, 0, hidden_size))\n",
        "            key_map[f\"transformer.{prefix_from}.attn.to_k{end}\"] = (linear1, (0, hidden_size, hidden_size))\n",
        "            key_map[f\"transformer.{prefix_from}.attn.to_v{end}\"] = (linear1, (0, hidden_size * 2, hidden_size))\n",
        "            key_map[f\"transformer.{prefix_from}.proj_mlp{end}\"] = (linear1, (0, hidden_size * 3, hidden_size * 4))\n",
        "\n",
        "            # Map other components\n",
        "            other_map = {\n",
        "                f\"{prefix_from}.proj_out{end}\": f\"{prefix}.linear2{clean_end}\",\n",
        "                f\"{prefix_from}.norm.linear{end}\": f\"{prefix}.modulation.lin{clean_end}\",\n",
        "            }\n",
        "            key_map.update(other_map)\n",
        "            key_map.update({f\"transformer.{k}\": v for k, v in other_map.items()})\n",
        "\n",
        "    # Map base model components\n",
        "    for end in [\".weight\", \".bias\", \".lora_A.weight\", \".lora_B.weight\"]:\n",
        "        clean_end = end.replace(\"lora_A\", \"A\").replace(\"lora_B\", \"B\")\n",
        "        base_map = {\n",
        "            f\"context_embedder{end}\": f\"{output_prefix}txt_in{clean_end}\",\n",
        "            f\"time_text_embed.timestep_embedder.linear_1{end}\": f\"{output_prefix}time_in.in_layer{clean_end}\",\n",
        "            f\"time_text_embed.timestep_embedder.linear_2{end}\": f\"{output_prefix}time_in.out_layer{clean_end}\",\n",
        "            f\"time_text_embed.text_embedder.linear_1{end}\": f\"{output_prefix}vector_in.in_layer{clean_end}\",\n",
        "            f\"time_text_embed.text_embedder.linear_2{end}\": f\"{output_prefix}vector_in.out_layer{clean_end}\",\n",
        "            f\"proj_out{end}\": f\"{output_prefix}final_layer.linear{clean_end}\",\n",
        "        }\n",
        "        key_map.update(base_map)\n",
        "        key_map.update({f\"transformer.{k}\": v for k, v in base_map.items()})\n",
        "\n",
        "    return key_map\n",
        "\n",
        "# Replace the original function\n",
        "totoro.utils.flux_to_diffusers = patched_flux_to_diffusers\n",
        "\n",
        "print(\"Patched Flux LoRA compatibility layer with correct component mapping\")\n",
        "\n",
        "\n",
        "# Helper function to check if a token is valid\n",
        "def is_valid_token(token):\n",
        "    placeholder_values = [\"YOUR_TOKEN_HERE\", \"YOUR_HUGGINGFACE_TOKEN_HERE\", \"\", None]\n",
        "    return token not in placeholder_values\n",
        "\n",
        "# Helper function to list available LoRAs from Google Drive\n",
        "def list_drive_loras(lora_dir):\n",
        "    if not os.path.exists(lora_dir):\n",
        "        return []\n",
        "    return [f for f in os.listdir(lora_dir) if f.endswith('.safetensors')]\n",
        "\n",
        "# Modified checkpoint loading logic\n",
        "if CHECKPOINT_SOURCE.lower() == \"googledrive\":\n",
        "    if not os.path.exists(CHECKPOINT_DRIVE_PATH):\n",
        "        raise Exception(f\"Checkpoint not found at: {CHECKPOINT_DRIVE_PATH}\")\n",
        "    # Copy checkpoint to ComfyUI directory\n",
        "    checkpoint_dest = \"/content/TotoroUI/models/checkpoints/full-checkpoint.safetensors\"\n",
        "    os.makedirs(os.path.dirname(checkpoint_dest), exist_ok=True)\n",
        "    shutil.copy2(CHECKPOINT_DRIVE_PATH, checkpoint_dest)\n",
        "    print(f\"Copied checkpoint from Google Drive to: {checkpoint_dest}\")\n",
        "elif CHECKPOINT_SOURCE.lower() == \"civitai\":\n",
        "    if not is_valid_token(CIVITAI_TOKEN):\n",
        "        print(\"Warning: No Civitai token provided. Attempting download in case model is public...\")\n",
        "    download_url = f\"{CHECKPOINT_CAI_URL}{'?token=' + CIVITAI_TOKEN if is_valid_token(CIVITAI_TOKEN) else ''}\"\n",
        "    download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d /content/TotoroUI/models/checkpoints -o full-checkpoint.safetensors'\n",
        "    os.system(download_cmd)\n",
        "else:  # huggingface\n",
        "    if not is_valid_token(HF_TOKEN):\n",
        "        print(\"Warning: No Hugging Face token provided. Attempting download in case model is public...\")\n",
        "    download_url = CHECKPOINT_HF_URL\n",
        "    download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d /content/TotoroUI/models/checkpoints -o full-checkpoint.safetensors'\n",
        "    if is_valid_token(HF_TOKEN):\n",
        "        download_cmd += f' --header=\"Authorization: Bearer {HF_TOKEN}\"'\n",
        "    os.system(download_cmd)\n",
        "\n",
        "\n",
        "CheckpointLoaderSimple = NODE_CLASS_MAPPINGS[\"CheckpointLoaderSimple\"]()\n",
        "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
        "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "\n",
        "with torch.inference_mode():\n",
        "    unet, clip, vae = CheckpointLoaderSimple.load_checkpoint(\"full-checkpoint.safetensors\")\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "downloaded_loras = {}\n",
        "\n",
        "# Modified download_lora function with filename collision handling\n",
        "def download_lora(url, source):\n",
        "    global downloaded_loras\n",
        "\n",
        "    cache_key = f\"{source}:{url}\"\n",
        "    if cache_key in downloaded_loras:\n",
        "        if os.path.exists(downloaded_loras[cache_key]):\n",
        "            print(f\"Using cached LoRA: {downloaded_loras[cache_key]}\")\n",
        "            return downloaded_loras[cache_key]\n",
        "        else:\n",
        "            # Cached file no longer exists, remove from cache\n",
        "            del downloaded_loras[cache_key]\n",
        "\n",
        "    def get_unique_filepath(base_path, filename):\n",
        "        \"\"\"Generate unique filepath by appending source and counter if needed\"\"\"\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        filepath = os.path.join(base_path, filename)\n",
        "        counter = 1\n",
        "\n",
        "        while os.path.exists(filepath):\n",
        "            # Check if it's literally the same file\n",
        "            if any(existing_path == filepath for existing_path in downloaded_loras.values()):\n",
        "                print(f\"Reusing identical LoRA path: {filepath}\")\n",
        "                return filepath\n",
        "\n",
        "            new_name = f\"{name}_{source}_{counter}{ext}\"\n",
        "            filepath = os.path.join(base_path, new_name)\n",
        "            counter += 1\n",
        "\n",
        "        return filepath\n",
        "\n",
        "    try:\n",
        "        if source.lower() == \"googledrive\":\n",
        "            # For Google Drive, url is actually the path to the LoRA file\n",
        "            if not os.path.exists(url):\n",
        "                raise Exception(f\"LoRA not found at: {url}\")\n",
        "            filename = os.path.basename(url)\n",
        "            filepath = get_unique_filepath('/content/TotoroUI/models/loras', filename)\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "            shutil.copy2(url, filepath)\n",
        "            print(f\"LoRA path: {filepath}\")\n",
        "            downloaded_loras[cache_key] = filepath\n",
        "            return filepath\n",
        "        elif source.lower() == \"civitai\":\n",
        "            if not is_valid_token(CIVITAI_TOKEN):\n",
        "                print(\"Warning: No Civitai token provided. Attempting LoRA download in case model is public...\")\n",
        "            headers = {\"Authorization\": f\"Bearer {CIVITAI_TOKEN}\"} if is_valid_token(CIVITAI_TOKEN) else {}\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                content_disposition = response.headers.get('Content-Disposition')\n",
        "                if content_disposition:\n",
        "                    filename = content_disposition.split('filename=')[-1].strip('\"').split(\"?\")[0]\n",
        "                else:\n",
        "                    filename = f\"lora_{len(downloaded_loras)}.safetensors\"\n",
        "                filepath = get_unique_filepath('/content/TotoroUI/models/loras', filename)\n",
        "                os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "                download_url = f\"{url}{'?token=' + CIVITAI_TOKEN if is_valid_token(CIVITAI_TOKEN) else ''}\"\n",
        "\n",
        "                download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{download_url}\" -d /content/TotoroUI/models/loras -o \"{os.path.basename(filepath)}\"'\n",
        "                os.system(download_cmd)\n",
        "\n",
        "                if not os.path.exists(filepath):\n",
        "                    raise Exception(f\"Download failed: {filename} not found. If this is a private model, make sure to provide the appropriate token.\")\n",
        "\n",
        "                print(f\"LoRA downloaded to: {filepath}\")\n",
        "                downloaded_loras[cache_key] = filepath\n",
        "                return filepath\n",
        "        else:  # huggingface\n",
        "            os.makedirs(\"/content/TotoroUI/models/loras\", exist_ok=True)\n",
        "            filename = url.split(\"/\")[-1] # Filename at end of HF URL\n",
        "            filepath = get_unique_filepath('/content/TotoroUI/models/loras', filename)\n",
        "\n",
        "            download_cmd = f'aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d /content/TotoroUI/models/loras -o \"{os.path.basename(filepath)}\"'\n",
        "            if is_valid_token(HF_TOKEN):\n",
        "                download_cmd += f' --header=\"Authorization: Bearer {HF_TOKEN}\"'\n",
        "            else:\n",
        "                print(\"Warning: No Hugging Face token provided. Attempting LoRA download in case model is public...\")\n",
        "\n",
        "            os.system(download_cmd)\n",
        "\n",
        "            if not os.path.exists(filepath):\n",
        "                raise Exception(f\"Download failed: {filename} not found. If this is a private model, make sure to provide the appropriate token.\")\n",
        "\n",
        "            print(f\"LoRA downloaded to: {filepath}\")\n",
        "            downloaded_loras[cache_key] = filepath\n",
        "            return filepath\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading LoRA: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance,\n",
        "             lora1_source, lora1_url, lora1_strength_model, lora1_strength_clip,\n",
        "             lora2_source, lora2_url, lora2_strength_model, lora2_strength_clip, clip_skip):\n",
        "    global unet, clip, vae\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(f\"Using seed: {seed}\")\n",
        "\n",
        "    try:\n",
        "        # LoRA handling\n",
        "        unet_lora, clip_lora = unet, clip\n",
        "\n",
        "        if lora1_url:\n",
        "            lora1_path = download_lora(lora1_url.split(\"?\")[0], lora1_source)\n",
        "            lora1_filename = os.path.basename(lora1_path)\n",
        "            print(f\"Using LoRA 1: {lora1_filename} from {lora1_source}\")\n",
        "            unet_lora, clip_lora = LoraLoader.load_lora(unet_lora, clip_lora, lora1_filename, lora1_strength_model, lora1_strength_clip)\n",
        "\n",
        "        if lora2_url:\n",
        "            lora2_path = download_lora(lora2_url.split(\"?\")[0], lora2_source)\n",
        "            lora2_filename = os.path.basename(lora2_path)\n",
        "            print(f\"Using LoRA 2: {lora2_filename} from {lora2_source}\")\n",
        "            unet_lora, clip_lora = LoraLoader.load_lora(unet_lora, clip_lora, lora2_filename, lora2_strength_model, lora2_strength_clip)\n",
        "\n",
        "        if not lora1_url and not lora2_url:\n",
        "            print(\"No LoRA URLs provided, using base model\")\n",
        "\n",
        "        # Encode the prompt\n",
        "        tokens = clip_lora.tokenize(positive_prompt)\n",
        "        cond, pooled = clip_lora.encode_from_tokens(tokens, return_pooled=True)\n",
        "\n",
        "        # Apply CLIP skip if needed\n",
        "        if clip_skip > 0:\n",
        "            cond = cond[:, :cond.shape[1] - clip_skip]\n",
        "\n",
        "        # Create proper conditioning structure expected by FluxGuidance\n",
        "        cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "        cond = FluxGuidance.append(cond, guidance)[0]\n",
        "\n",
        "        # Set up sampling parameters\n",
        "        noise = RandomNoise.get_noise(seed)[0]\n",
        "        guider = BasicGuider.get_guider(unet_lora, cond)[0]\n",
        "        sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "        sigmas = BasicScheduler.get_sigmas(unet_lora, scheduler, steps, 1.0)[0]\n",
        "\n",
        "        # Create latent noise image\n",
        "        latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "\n",
        "        # Perform sampling\n",
        "        sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "        decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "        Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(\"/content/flux.png\")\n",
        "        return \"/content/flux.png\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "# Modified Gradio interface with complete UI handling\n",
        "with gr.Blocks(analytics_enabled=False) as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            positive_prompt = gr.Textbox(lines=3, interactive=True,\n",
        "                                       value=\"Intergalactic beauty contest winner, pretty female humanoid alien in party dress, holding up sign saying 'Full Flux With 2 LoRA Colab'\",\n",
        "                                       label=\"Prompt\")\n",
        "            width = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"width\")\n",
        "            height = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"height\")\n",
        "            seed = gr.Slider(minimum=0, maximum=18446744073709551615, value=0, step=1, label=\"seed (0=random)\")\n",
        "            steps = gr.Slider(minimum=4, maximum=50, value=20, step=1, label=\"steps\")\n",
        "            guidance = gr.Slider(minimum=0, maximum=20, value=3.5, step=0.5, label=\"guidance\")\n",
        "            sampler_name = gr.Dropdown([\"euler\", \"heun\", \"heunpp2\", \"lms\", \"dpm_2\", \"dpmpp_2m\", \"deis\", \"ddim\", \"uni_pc\", \"uni_pc_bh2\"],\n",
        "                                     label=\"sampler_name\", value=\"dpmpp_2m\")\n",
        "            scheduler = gr.Dropdown([\"normal\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"],\n",
        "                                  label=\"scheduler\", value=\"sgm_uniform\")\n",
        "\n",
        "\n",
        "\n",
        "            # Modified LoRA 1 settings with Google Drive support\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### LoRA 1 Settings\")\n",
        "                lora1_source = gr.Radio([\"civitai\", \"huggingface\", \"googledrive\"],\n",
        "                                      label=\"LoRA 1 Source\", value=\"civitai\")\n",
        "                lora1_drive_files = gr.Dropdown(choices=[], label=\"Select LoRA 1 from Drive\", visible=False)\n",
        "                lora1_url = gr.Textbox(lines=1, interactive=True, value=\"\", label=\"LoRA 1 URL/Path\")\n",
        "                lora1_strength_model = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                               label=\"LoRA 1 Strength (Model)\")\n",
        "                lora1_strength_clip = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                              label=\"LoRA 1 Strength (CLIP)\")\n",
        "\n",
        "            # Modified LoRA 2 settings with Google Drive support\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### LoRA 2 Settings\")\n",
        "                lora2_source = gr.Radio([\"civitai\", \"huggingface\", \"googledrive\"],\n",
        "                                      label=\"LoRA 2 Source\", value=\"civitai\")\n",
        "                lora2_drive_files = gr.Dropdown(choices=[], label=\"Select LoRA 2 from Drive\", visible=False)\n",
        "                lora2_url = gr.Textbox(lines=1, interactive=True, value=\"\", label=\"LoRA 2 URL/Path\")\n",
        "                lora2_strength_model = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                               label=\"LoRA 2 Strength (Model)\")\n",
        "                lora2_strength_clip = gr.Slider(minimum=-1.0, maximum=1.5, value=1.0, step=0.05,\n",
        "                                              label=\"LoRA 2 Strength (CLIP)\")\n",
        "\n",
        "            clip_skip = gr.Slider(minimum=0, maximum=2, value=0, step=1, label=\"CLIP Skip\")\n",
        "            generate_button = gr.Button(\"Generate\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_image = gr.Image(label=\"Generated image\", interactive=False)\n",
        "\n",
        "    # Updated interface update functions\n",
        "    def update_lora_interface(source, url_textbox):\n",
        "        \"\"\"Update LoRA interface based on selected source\"\"\"\n",
        "        if source == \"googledrive\" and 'LORA_DRIVE_DIR' in globals():\n",
        "            return (\n",
        "                gr.update(visible=True, choices=list_drive_loras(LORA_DRIVE_DIR), value=None),\n",
        "                gr.update(visible=True, interactive=False, value=\"\")\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                gr.update(visible=False, choices=[], value=None),\n",
        "                gr.update(visible=True, interactive=True, value=\"\")\n",
        "            )\n",
        "\n",
        "    def update_lora_url(selected_file, source):  # Added source parameter\n",
        "        \"\"\"Update URL when a file is selected from dropdown\"\"\"\n",
        "        if selected_file and source == \"googledrive\":  # Only modify interactivity for Google Drive\n",
        "            full_path = f\"{LORA_DRIVE_DIR}/{selected_file}\"\n",
        "            return gr.update(value=full_path, visible=True, interactive=False)\n",
        "        return gr.update(value=\"\", visible=True)  # Maintain current interactive state for other sources\n",
        "\n",
        "    # Set up LoRA 1 interface events\n",
        "    lora1_source.change(\n",
        "        fn=update_lora_interface,\n",
        "        inputs=[lora1_source, lora1_url],\n",
        "        outputs=[lora1_drive_files, lora1_url]\n",
        "    )\n",
        "    lora1_drive_files.change(\n",
        "        fn=update_lora_url,\n",
        "        inputs=[lora1_drive_files, lora1_source],  # Added source input\n",
        "        outputs=[lora1_url]\n",
        "    )\n",
        "\n",
        "    # Set up LoRA 2 interface events\n",
        "    lora2_source.change(\n",
        "        fn=update_lora_interface,\n",
        "        inputs=[lora2_source, lora2_url],\n",
        "        outputs=[lora2_drive_files, lora2_url]\n",
        "    )\n",
        "    lora2_drive_files.change(\n",
        "        fn=update_lora_url,\n",
        "        inputs=[lora2_drive_files, lora2_source],  # Added source input\n",
        "        outputs=[lora2_url]\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        fn=generate,\n",
        "        inputs=[\n",
        "            positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance,\n",
        "            lora1_source, lora1_url, lora1_strength_model, lora1_strength_clip,\n",
        "            lora2_source, lora2_url, lora2_strength_model, lora2_strength_clip, clip_skip\n",
        "        ],\n",
        "        outputs=output_image\n",
        "    )\n",
        "\n",
        "demo.queue().launch(inline=False, share=True, debug=True)"
      ],
      "metadata": {
        "id": "_g4D83wKJqrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
